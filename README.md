# Local-Inference-Server
This contains the code I used to setup a local inference server during my internship as well as a basic requests library use case for your local PC
A local inference server is a type of computing infrastructure that allows you to run machine learning models directly on your own local hardware, rather than relying on a remote cloud-based service.
This code uses FastAPI and involves certain docker implementation as well(you can google how to integrate that)
To run this, you will also need to download Ubuntu, Windows Subsystem for Linux and a locally host a Large Language Model (I hosted Mistral 7B for my project after downloading the pre trained model from Hugging Face Model Hub), you can download the dependencies of your LLM locally using pip.
